\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=2cm]
\node (data_in) [io] {Data};
\node (xs_in) [io, right=0.5em of data_in] {Xs};
\node (ys_in) [io, right=0.5em of xs_in] {Ys};
\node (dep_in) [io, right=0.5em of ys_in] {dep};
\node (L1) [decision, below=0.5em of xs_in] {L1: Collect size?};
\draw [arrow] (data_in) -- (L1);
\draw [arrow] (xs_in) -- (L1);
\draw [arrow] (ys_in) -- (L1);
\draw [arrow] (dep_in) -- (L1);
\node (collect_yes) [process, below=3em of data_in] {$S \leftarrow \text{Count}_\epsilon(Data)$};
\node (collect_no) [io, below=3em of dep_in] {$S \leftarrow \infty$};
\draw [arrow] (L1) -- node {yes} (collect_yes);
\draw [arrow] (L1) -- node {no} (collect_no);
\node (stop) [decision, below=6em of xs_in] {dep=0 or $S < c$?};
\draw [arrow] (collect_no) -- node {yes} (stop);
\draw [arrow] (collect_yes) -- node {no} (stop);
\node (stop_yes) [process, below=1em of stop] {Leaf(most common Ys)};
\node (stop_no) [decision, right=2em of stop] {L2: Use Exp Mech?};
\draw [arrow] (stop) -- node {yes} (stop_yes);
\draw [arrow] (stop) -- node {no} (stop_no);
\node (exp_yes) [process, below=2em of stop_no] {$B \leftarrow \text{exp\_mech}_\epsilon$};
\node (exp_no) [io, right=0.5em of exp_yes] {$B \leftarrow \text{rand}$};
\draw [arrow] (stop_no) -- node {yes} (exp_yes);
\draw [arrow] (stop_no) -- node {no} (exp_no);
\node (wrap_up) [io, below=1em of exp_yes] {For each $D \in \text{Split}(Data, B)$: do $DTree(D, Xs\setminus B, Ys, dep-1)$};
\draw [arrow] (exp_yes) -- (wrap_up);
\draw [arrow] (exp_no) -- (wrap_up);
\end{tikzpicture}
\end{center}
\caption{Our decision tree algorithm with NoisyConditionals which learn the execution path from past histories. Depending on what the NoisyConditionals say, this algorithm is capable of expressing all the decision tree algorithms in~\cite{Fletcher:2016}.}\label{alg:dtree}
\end{figure}