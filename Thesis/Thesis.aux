\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Dwork:2006}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Narayanan:2006}
\citation{Hay:2016}
\citation{Li:2014}
\citation{Li:2014}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A screenshot of a DPComp graph depicting the original dataset and the noise that a DP algorthm, DAWA \cite  {Li:2014}, adds at $\epsilon =0.01$. This would help an analyst decide whether to apply DAWA on his own dataset.}}{3}{figure.1}}
\newlabel{fig:dpcomp}{{1}{3}{A screenshot of a DPComp graph depicting the original dataset and the noise that a DP algorthm, DAWA \cite {Li:2014}, adds at $\epsilon =0.01$. This would help an analyst decide whether to apply DAWA on his own dataset}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Example \texttt  {NoisyIf} statement.}}{4}{figure.2}}
\newlabel{fig:1}{{2}{4}{Example \texttt {NoisyIf} statement}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{4}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Differential Privacy}{4}{subsection.2.1}}
\citation{Dwork:2006}
\citation{Dwork:2006}
\citation{Dwork:2006}
\newlabel{thm:comp}{{2}{5}{}{theorem.2}{}}
\citation{Dwork:2006}
\newlabel{thm:disj}{{3}{6}{}{theorem.3}{}}
\newlabel{alg:1}{{1}{6}{Differential Privacy}{algocfline.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Laplace Mechanism}}{6}{algocf.1}}
\citation{McSherry:2010}
\newlabel{alg:max}{{2}{7}{Differential Privacy}{algocfline.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces ReportNoisyMax}}{7}{algocf.2}}
\newlabel{alg:exp}{{3}{7}{Differential Privacy}{algocfline.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces Exponential Mechanism}}{7}{algocf.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Related Work}{7}{subsection.2.2}}
\citation{Proserpio:2014}
\citation{Johnson:2017}
\citation{Proserpio:2014}
\citation{Johnson:2017}
\citation{Reed:2010}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces NoisyCount Implemented in PINQ.}}{8}{figure.3}}
\newlabel{fig:PINQNoisyCount}{{3}{8}{NoisyCount Implemented in PINQ}{figure.3}{}}
\citation{Koufogiannis:2015}
\citation{Ligett:2017}
\citation{Dwork:2006}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Existing Methods for Algorithmic Choice}{9}{subsection.2.3}}
\citation{Winograd-Cort:2017}
\citation{Liu:2018}
\citation{Hsu:2014}
\citation{Fletcher:2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Decision Trees}{10}{subsection.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Example Decision Tree.}}{11}{figure.4}}
\newlabel{fig:dt}{{4}{11}{Example Decision Tree}{figure.4}{}}
\newlabel{eq:cond_ent}{{1}{11}{Decision Trees}{equation.2.1}{}}
\citation{Fletcher:2016}
\@writefile{lol}{\contentsline {lstlisting}{./DTree.py}{12}{lstlisting.-2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces C4.5 Algorithm.}}{12}{figure.5}}
\newlabel{alg:c45}{{5}{12}{C4.5 Algorithm}{figure.5}{}}
\newlabel{eq:gini}{{2}{12}{Decision Trees}{equation.2.2}{}}
\newlabel{eq:max}{{3}{12}{Decision Trees}{equation.2.3}{}}
\citation{Blum:2005}
\citation{Friedman:2010}
\@writefile{toc}{\contentsline {section}{\numberline {3}Decision Tree Examples}{13}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Private Decision Trees}{13}{subsection.3.1}}
\newlabel{eq:priv_est}{{4}{13}{Private Decision Trees}{equation.3.4}{}}
\newlabel{thm:ent_sens}{{5}{13}{}{theorem.5}{}}
\citation{Fletcher:2016}
\citation{Friedman:2010}
\citation{Quinlan:1993}
\citation{Friedman:2010}
\citation{Friedman:2010}
\citation{Mohammed:2015}
\citation{Jagannathan:2009}
\citation{Singh:2014}
\citation{Fletcher:2015}
\@writefile{lol}{\contentsline {lstlisting}{./DTree.py}{15}{lstlisting.-3}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Private C4.5 proposed by Friedman and Schuster \cite  {Friedman:2010}. }}{15}{figure.6}}
\newlabel{alg:pc45}{{6}{15}{Private C4.5 proposed by Friedman and Schuster \cite {Friedman:2010}}{figure.6}{}}
\citation{Mohammed:2015}
\citation{Jagannathan:2009}
\citation{Singh:2014}
\citation{Fletcher:2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Experiments}{16}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Performances of the five decision tree algorithms. The performance is measured from the prediction success rate on a validation set using a 30/70 validation to training split. Graph 1 does not have A5 because it takes such a long time to train, having a weak stopping criterion.}}{18}{figure.7}}
\newlabel{fig:datadep}{{7}{18}{Performances of the five decision tree algorithms. The performance is measured from the prediction success rate on a validation set using a 30/70 validation to training split. Graph 1 does not have A5 because it takes such a long time to train, having a weak stopping criterion}{figure.7}{}}
\citation{Fletcher:2016}
\citation{Fletcher:2016}
\bibstyle{plain}
\bibdata{Thesis}
\bibcite{Blum:2005}{1}
\bibcite{Dwork:2006}{2}
\bibcite{Fletcher:2015}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Our decision tree algorithm with NoisyConditionals which learn the execution path from past histories. Depending on what the NoisyConditionals say, this algorithm is capable of expressing all the decision tree algorithms in \cite  {Fletcher:2016}.}}{19}{figure.8}}
\newlabel{alg:dtree}{{8}{19}{Our decision tree algorithm with NoisyConditionals which learn the execution path from past histories. Depending on what the NoisyConditionals say, this algorithm is capable of expressing all the decision tree algorithms in \cite {Fletcher:2016}}{figure.8}{}}
\bibcite{Fletcher:2016}{4}
\bibcite{Friedman:2010}{5}
\bibcite{Hay:2016}{6}
\bibcite{Hsu:2014}{7}
\bibcite{Jagannathan:2009}{8}
\bibcite{Johnson:2017}{9}
\bibcite{Koufogiannis:2015}{10}
\bibcite{Li:2014}{11}
\bibcite{Ligett:2017}{12}
\bibcite{Liu:2018}{13}
\bibcite{McSherry:2010}{14}
\bibcite{Mohammed:2015}{15}
\bibcite{Narayanan:2006}{16}
\bibcite{Proserpio:2014}{17}
\bibcite{Quinlan:1993}{18}
\bibcite{Reed:2010}{19}
\bibcite{Singh:2014}{20}
\bibcite{Winograd-Cort:2017}{21}
